{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "* AKA tool calling \n",
    "* Function calling is a way to connect an AI model with external tools.\n",
    "    * With all models, there are limitations to real-time information and risks of hallucination when prompted with complex questions. \n",
    "* Function calling allows you to connect your own functions within the chat completion API call by using the “tools” and “tool choice” parameters. \n",
    "    * tool_choice: Directs the model on which (if any) function to choose.\n",
    "        * “none” or not defining this parameter makes the model not call a function and acts as a normal chat completion API call\n",
    "        * “auto” makes the model decide on which function to call based on the context of the prompt\n",
    "        * For a specific function call, you have to pass the function name in a dictionary format. This will force the model to choose that function.\n",
    "        { \"type\": \"function\", \"function\": { \"name\": \"my_function\" }}\n",
    "    * tools: When “tool choice” is set to “auto” or a specific function name, the model will refer to the “tools” parameter.\n",
    "        * This holds the list of available tools to choose from.\n",
    "        * To list the tools properly for the model to make the proper understanding of the function and its arguments, the tool list is formatted in a dictionary with natural language description of the overall function and each argument with its relationship to the function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't ran these pip install functions yet, run this code block.\n",
    "If you've already installed these libraries, there's no need to run this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\sarah\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openaiNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\sarah\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script distro.exe is installed in 'c:\\Users\\sarah\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'c:\\Users\\sarah\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script openai.exe is installed in 'c:\\Users\\sarah\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading openai-1.35.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.8.0-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.5/123.5 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting typing-extensions<5,>=4.7 (from openai)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.20.0-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarah\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.35.8-py3-none-any.whl (328 kB)\n",
      "   ---------------------------------------- 0.0/328.1 kB ? eta -:--:--\n",
      "   ---------------------------- ----------- 235.5/328.1 kB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 328.1/328.1 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "   ---------------------------------------- 0.0/86.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 86.8/86.8 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.9/77.9 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.8.0-py3-none-any.whl (423 kB)\n",
      "   ---------------------------------------- 0.0/423.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 423.1/423.1 kB 8.8 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.20.0-cp312-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.9 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 11.0 MB/s eta 0:00:00\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "   ---------------------------------------- 0.0/66.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 66.8/66.8 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 164.4/164.4 kB 10.3 MB/s eta 0:00:00\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: typing-extensions, tqdm, sniffio, idna, h11, distro, certifi, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.4.0 certifi-2024.6.2 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 openai-1.35.8 pydantic-2.8.0 pydantic-core-2.20.0 sniffio-1.3.1 tqdm-4.66.4 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libaries and load in the API key\n",
    "* This is the same thing as we did for the chat completions API\n",
    "* This is a way to safely access the API key and not have it hard coded into the script\n",
    "* Notice that we also import the 'json' library. We will be using that on the response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os, json\n",
    "\n",
    "# I have my API key stored in a file called api.env\n",
    "# The file is in the same directory as this script\n",
    "\n",
    "# If the print statement returns None, then the API key is not being read\n",
    "# Try using the full path to the file, and switching the slashes to forward slashes\n",
    "\n",
    "load_dotenv(\"api.env\") \n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the API client and create the message list\n",
    "* The API client is basically the portal into the available API services/data\n",
    "* We use the API key as essentially a password to gain access to the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the connection to the OpenAI API using the API key\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a prompt for the user to enter a message\u001b[39;00m\n\u001b[0;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a message: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the connection to the OpenAI API using the API key\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "# Create a prompt for the user to enter a message\n",
    "prompt = input(\"Enter a message: \")\n",
    "\n",
    "# Create a list of messages to send to the API\n",
    "message_list = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": prompt\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions and a list of function descriptions\n",
    "* We create the 'def' functions we want the model to be able to call\n",
    "* Then we create a list of dictionaries which are natural language descriptions of the function and its arguments \n",
    "\n",
    "## Dictionary Schema for 'tools' parameter\n",
    "    * type: The type of the tool. Currently only 'function' is supported.\n",
    "    * function: A dictionary object that holds the details of the function\n",
    "        * name: The name of the function. Has to be the same name as the corresponding 'def' function.\n",
    "        * description: A description of what the function does.\n",
    "        * parameters: A dictionary object that holds the details of the arguments\n",
    "            * type: Will always be \"object\"\n",
    "            * properties: A dictionary object that holds the details of each argument\n",
    "                * name_of_the_argument: A dictionary object that holds details of this specific argument\n",
    "                    * type: The type of data that this argument will be. I believe this only supports 'string' or 'object'.\n",
    "                    * description: A description of what this argument is and how it relates to the function it is part of. It's always helpful to give an example.\n",
    "                    * enum: You can create a list for the model to choose from if there are specific values you are looking for.\n",
    "            * required: A list of arguments that are required. This allows for us to have arguments listed that don't need to be called if they aren't specified in the user prompt. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the actual function that the model's response object will call\n",
    "# Notice how unit is an optional parameter (already initialized to \"fahrenheit\") \n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    # Usually there would be more code here to get the weather data\n",
    "    # But for now, we'll just return a set string saying that it is 80 degrees (possibly) fahrenheit at every location you give it\n",
    "    return f\"The current weather in {location} is 80 degrees {unit}.\"\n",
    "\n",
    "# This is the list of dictionaries holding the natural language description of the function and the function's arguments\n",
    "# This object has a specific schema (look in the notes above this codeblock) that needs to be followed \n",
    "tool_options = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\", # Notice how we give the model an example of the string that it should generate for this argument\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"] # Enum allows us to give a list of options the model can choose from instead of generating it's own value\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"], # We set 'location' to be required and not 'unit' because unit is an optional argument that is initialized to 'fahrenheit' and the function will still properly run without it in the arguments.\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a tool_option object that holds multiple functions in it\n",
    "# Notice how the last function doesn't have any arguments\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"fahrenheit\", \"celcius\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"ask_wolfram\",\n",
    "            \"description\": \"Ask Wolfram Alpha a question for factual information.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The question to ask Wolfram Alpha.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_time_and_date\",\n",
    "            \"description\": \"Get the current time and date.\",\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the API call\n",
    "* This is the chat completions API still, but we are adding the two parameters that allow it make function calls.\n",
    "* model and messages are required parameters in the chat completions API\n",
    "* tool_choice: Can only be one of three options ('none', 'auto', { \"type\": \"function\", \"function\": { \"name\": \"your_function_name\" }})\n",
    "    * I set this one to 'auto' with only one function detailed in 'tool_options' so that the model can choose whether to respond in natural language or with a function call.\n",
    "    * 'none' will make it so that it will only respond in natural language, and with a dictionary like the one above, the model will always respond with that function call and its predicted arguments.\n",
    "* tools: Takes the list of dictionaries object that holds the natural language description of the functions for the model to know what it can choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=message_list,\n",
    "    tool_choice=\"auto\",\n",
    "    tools=tools,\n",
    "    # temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigate the API response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_PKUWdlVpzy3XqEA05QlQ6TUB', function=Function(arguments='{\"location\":\"Miami\",\"unit\":\"fahrenheit\"}', name='get_current_weather'), type='function')]))]\n",
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_PKUWdlVpzy3XqEA05QlQ6TUB', function=Function(arguments='{\"location\":\"Miami\",\"unit\":\"fahrenheit\"}', name='get_current_weather'), type='function')])\n",
      "None\n",
      "[ChatCompletionMessageToolCall(id='call_PKUWdlVpzy3XqEA05QlQ6TUB', function=Function(arguments='{\"location\":\"Miami\",\"unit\":\"fahrenheit\"}', name='get_current_weather'), type='function')]\n",
      "Function(arguments='{\"location\":\"Miami\",\"unit\":\"fahrenheit\"}', name='get_current_weather')\n",
      "get_current_weather\n",
      "{\"location\":\"Miami\",\"unit\":\"fahrenheit\"}\n",
      "Miami\n"
     ]
    }
   ],
   "source": [
    "# This is the whole response object from the API\n",
    "# print(completion)\n",
    "# Then we look at the choices that the model made\n",
    "print(completion.choices)\n",
    "# # We look at the message details that the model created\n",
    "print(completion.choices[0].message)\n",
    "# # We look at the natural language response from the model (this will be 'None' if it makes a function call)\n",
    "print(completion.choices[0].message.content)\n",
    "# # We look at the tool calls that the model decided to make\n",
    "print(completion.choices[0].message.tool_calls)\n",
    "# # We look at the first tool call and the function details \n",
    "print(completion.choices[0].message.tool_calls[0].function)\n",
    "# # We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\n",
    "print(completion.choices[0].message.tool_calls[0].function.name)\n",
    "# # We look at the function's arguments that the model chose. This is formatted as a JSON but is currently a string.\n",
    "print(completion.choices[0].message.tool_calls[0].function.arguments)\n",
    "\n",
    "# # We use the 'json' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\n",
    "arguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\n",
    "print(arguments[\"location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write some code that will take an API response object and do one of two things.\n",
    "1. If the model responds with natural language, we display to the user using the print statement (easy)\n",
    "2. If the model responds with a function call, we get the function details and run that function in our script (not as easy)\n",
    "\n",
    "Note: It is helpful to make variables for certain sections of the response object so that way the code is a bit easier to read and comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Miami is 80 degrees fahrenheit.\n"
     ]
    }
   ],
   "source": [
    "# Make variables for the natural language response or the tool call object that the model will generate\n",
    "answer = completion.choices[0].message.content\n",
    "tool_calls = completion.choices[0].message.tool_calls\n",
    "\n",
    "# This code works because OpenAI's models respond with EITHER a natural language response or a function call. There are other models that can generate both in the same response object. \n",
    "\n",
    "# If tool_calls isn't empty (for natural language responses, it will be 'None')\n",
    "if tool_calls:\n",
    "    # For every tool call in the tool calls object (there usually is only one, but there could be more with parallel function calling)\n",
    "    for tool_call in tool_calls:\n",
    "        # Finds the function called and the arguments passed to the function from the API response\n",
    "        function_called = tool_call.function.name\n",
    "        function_args = tool_call.function.arguments\n",
    "        # Converts the arguments to a JSON object instead of a JSON string\n",
    "        function_args_json = json.loads(function_args)\n",
    "        # Calls the function with the arguments\n",
    "        # globals() is a Python native function that returns a dictionary with the global variables (including functions)\n",
    "        if function_called in globals():\n",
    "            # We call the function using eval which is another Python native function that \"evaluates\" based off it's knowledge of the global and local variables\n",
    "            # (**function_args_json) is a way to pass the JSON object as arguments \n",
    "            # result holds the returned value of the function that was ran\n",
    "            result = eval(f\"{function_called}(**function_args_json)\")\n",
    "        # If there is no function by that name in the script, it will return an error message. Double check that the function name in the script and in the tool_options object are the same.\n",
    "        else:\n",
    "            result = \"Function not found. Please try again.\"\n",
    "        # This prints either the returned value of the successful function call or the error message saying it couldn't find the function in the script\n",
    "        print(result)\n",
    "# If tool_calls is empty, that means we just print out the natural language response from the model using the variable we created at the top\n",
    "else:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  \"get_ipython().run_line_magic('pip', 'install python-dotenv')\\nget_ipython().run_line_magic('pip', 'install openai')\",\n",
       "  'from dotenv import load_dotenv\\nfrom openai import OpenAI\\nimport os, json\\n\\nload_dotenv(\"api.env\")  # could pass in the path of the .env file in the arguments\\nprint(os.getenv(\"OPENAI_API_KEY\"))',\n",
       "  'def get_current_weather(location, unit=\"fahrenheit\"):\\n    return f\"The current weather in {location} is 80 degrees {unit}.\"\\n\\ntool_options = [\\n    {\\n        \"type\": \"function\",\\n        \"function\": {\\n            \"name\": \"get_current_weather\",\\n            \"description\": \"Get the current weather in a given location\",\\n            \"parameters\": {\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"location\": {\\n                        \"type\": \"int\",\\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\\n                    },\\n                    \"unit\": {\\n                        \"type\": \"string\",\\n                        \"enum\": [\"celsius\", \"fahrenheit\"]\\n                    },\\n                },\\n                \"required\": [\"location\"],\\n            },\\n        }\\n    }\\n]',\n",
       "  '# Create the connection to the OpenAI API using the API key\\nclient = OpenAI(api_key=os.environ.get(\\'OPENAI_API_KEY\\'))\\n\\n# Create a prompt for the user to enter a message\\nprompt = input(\"Enter a message: \")\\n\\n# Create a list of messages to send to the API\\nmessage_list = [\\n    {\\n        \"role\": \"system\", \\n        \"content\": \"You are a helpful assistant.\"\\n    },\\n    {\\n        \"role\": \"user\", \\n        \"content\": prompt\\n    }\\n]',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tools=tool_options,\\n    tool_choice=\"auto\"\\n)',\n",
       "  'print(completion)',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       "  '# This is the actual function that the model\\'s response object will call\\n# Notice how unit is an optional parameter (already initialized to \"fahrenheit\") \\ndef get_current_weather(location, unit=\"fahrenheit\"):\\n    # Usually there would be more code here to get the weather data\\n    # But for now, we\\'ll just return a set string saying that it is 80 degrees (possibly) fahrenheit at every location you give it\\n    return f\"The current weather in {location} is 80 degrees {unit}.\"\\n\\n# This is the list of dictionaries holding the natural language description of the function and the function\\'s arguments\\n# This object has a specific schema (look in the notes above this codeblock) that needs to be followed \\ntool_options = [\\n    {\\n        \"type\": \"function\",\\n        \"function\": {\\n            \"name\": \"get_current_weather\",\\n            \"description\": \"Get the current weather in a given location\",\\n            \"parameters\": {\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"location\": {\\n                        \"type\": \"string\",\\n                        \"description\": \"The city and state, e.g. San Francisco, CA\", # Notice how we give the model an example of the string that it should generate for this argument\\n                    },\\n                    \"unit\": {\\n                        \"type\": \"string\",\\n                        \"enum\": [\"celsius\", \"fahrenheit\"] # Enum allows us to give a list of options the model can choose from instead of generating it\\'s own value\\n                    },\\n                },\\n                \"required\": [\"location\"], # We set \\'location\\' to be required and not \\'unit\\' because unit is an optional argument that is initialized to \\'fahrenheit\\' and the function will still properly run without it in the arguments.\\n            },\\n        }\\n    }\\n]',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       "  'print(completion)',\n",
       "  'print(completion)\\n\\nwhat = json.load(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dunp(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dunps(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dump(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dumps(completion)\\nwhat',\n",
       "  'print(completion)',\n",
       "  'print(completion)\\nprint(completion.choices)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0])\\nprint(completion.choices[0].message)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0])\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\npritn(completion.choices[0].message.tool_calls[0].function.arguments)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)',\n",
       "  \"print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments['location'])\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)\",\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# Create the connection to the OpenAI API using the API key\\nclient = OpenAI(api_key=os.environ.get(\\'OPENAI_API_KEY\\'))\\n\\n# Create a prompt for the user to enter a message\\nprompt = input(\"Enter a message: \")\\n\\n# Create a list of messages to send to the API\\nmessage_list = [\\n    {\\n        \"role\": \"system\", \\n        \"content\": \"You are a helpful assistant.\"\\n    },\\n    {\\n        \"role\": \"user\", \\n        \"content\": prompt\\n    }\\n]',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options,\\n    temperature=0.0\\n)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options,\\n    # temperature=0.0\\n)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# This is the whole response object from the API\\nprint(completion + \\'\\\\n\\')\\n# Then we look at the choices that the model made\\nprint(completion.choices + \\'\\\\n\\')\\n# We look at the message details that the model created\\nprint(completion.choices[0].message + \\'\\\\n\\')\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content + \\'\\\\n\\')\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  'globals()',\n",
       "  '# Make variables for the natural language response or the tool call object that the model will generate\\nanswer = completion.choices[0].message.content\\ntool_calls = completion.choices[0].message.tool_calls\\n\\n# This code works because OpenAI\\'s models respond with EITHER a natural language response or a function call. There are other models that can generate both in the same response object. \\n\\n# If tool_calls isn\\'t empty (for natural language responses, it will be \\'None\\')\\nif tool_calls:\\n    # For every tool call in the tool calls object (there usually is only one, but there could be more with parallel function calling)\\n    for tool_call in tool_calls:\\n        # Finds the function called and the arguments passed to the function from the API response\\n        function_called = tool_call.function.name\\n        function_args = tool_call.function.arguments\\n        # Converts the arguments to a JSON object instead of a JSON string\\n        function_args_dict = json.loads(function_args)\\n        # Calls the function with the arguments\\n        # globals() is a Python native function that returns a dictionary with the global variables (including functions)\\n        if function_called in globals():\\n            result = eval(f\"{function_called}(**function_args_dict)\")\\n        else:\\n            result = \"Function not found. Please try again.\"\\n        print(result)\\nelse:\\n    print(answer)',\n",
       "  'globals()\\nf\"{function_called}(**function_args_json)\"',\n",
       "  'globals()'],\n",
       " '_oh': {37: {...}, 39: 'get_current_weather(**function_args_json)'},\n",
       " '_dh': [WindowsPath('c:/Users/Patrick/Desktop/PROJECTS/Python Lesson Plan/AI_CLASS_7')],\n",
       " 'In': ['',\n",
       "  \"get_ipython().run_line_magic('pip', 'install python-dotenv')\\nget_ipython().run_line_magic('pip', 'install openai')\",\n",
       "  'from dotenv import load_dotenv\\nfrom openai import OpenAI\\nimport os, json\\n\\nload_dotenv(\"api.env\")  # could pass in the path of the .env file in the arguments\\nprint(os.getenv(\"OPENAI_API_KEY\"))',\n",
       "  'def get_current_weather(location, unit=\"fahrenheit\"):\\n    return f\"The current weather in {location} is 80 degrees {unit}.\"\\n\\ntool_options = [\\n    {\\n        \"type\": \"function\",\\n        \"function\": {\\n            \"name\": \"get_current_weather\",\\n            \"description\": \"Get the current weather in a given location\",\\n            \"parameters\": {\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"location\": {\\n                        \"type\": \"int\",\\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\\n                    },\\n                    \"unit\": {\\n                        \"type\": \"string\",\\n                        \"enum\": [\"celsius\", \"fahrenheit\"]\\n                    },\\n                },\\n                \"required\": [\"location\"],\\n            },\\n        }\\n    }\\n]',\n",
       "  '# Create the connection to the OpenAI API using the API key\\nclient = OpenAI(api_key=os.environ.get(\\'OPENAI_API_KEY\\'))\\n\\n# Create a prompt for the user to enter a message\\nprompt = input(\"Enter a message: \")\\n\\n# Create a list of messages to send to the API\\nmessage_list = [\\n    {\\n        \"role\": \"system\", \\n        \"content\": \"You are a helpful assistant.\"\\n    },\\n    {\\n        \"role\": \"user\", \\n        \"content\": prompt\\n    }\\n]',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tools=tool_options,\\n    tool_choice=\"auto\"\\n)',\n",
       "  'print(completion)',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       "  '# This is the actual function that the model\\'s response object will call\\n# Notice how unit is an optional parameter (already initialized to \"fahrenheit\") \\ndef get_current_weather(location, unit=\"fahrenheit\"):\\n    # Usually there would be more code here to get the weather data\\n    # But for now, we\\'ll just return a set string saying that it is 80 degrees (possibly) fahrenheit at every location you give it\\n    return f\"The current weather in {location} is 80 degrees {unit}.\"\\n\\n# This is the list of dictionaries holding the natural language description of the function and the function\\'s arguments\\n# This object has a specific schema (look in the notes above this codeblock) that needs to be followed \\ntool_options = [\\n    {\\n        \"type\": \"function\",\\n        \"function\": {\\n            \"name\": \"get_current_weather\",\\n            \"description\": \"Get the current weather in a given location\",\\n            \"parameters\": {\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"location\": {\\n                        \"type\": \"string\",\\n                        \"description\": \"The city and state, e.g. San Francisco, CA\", # Notice how we give the model an example of the string that it should generate for this argument\\n                    },\\n                    \"unit\": {\\n                        \"type\": \"string\",\\n                        \"enum\": [\"celsius\", \"fahrenheit\"] # Enum allows us to give a list of options the model can choose from instead of generating it\\'s own value\\n                    },\\n                },\\n                \"required\": [\"location\"], # We set \\'location\\' to be required and not \\'unit\\' because unit is an optional argument that is initialized to \\'fahrenheit\\' and the function will still properly run without it in the arguments.\\n            },\\n        }\\n    }\\n]',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       "  'print(completion)',\n",
       "  'print(completion)\\n\\nwhat = json.load(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dunp(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dunps(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dump(completion)\\nwhat',\n",
       "  'print(completion)\\n\\nwhat = json.dumps(completion)\\nwhat',\n",
       "  'print(completion)',\n",
       "  'print(completion)\\nprint(completion.choices)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0])\\nprint(completion.choices[0].message)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0])\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\npritn(completion.choices[0].message.tool_calls[0].function.arguments)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)',\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)',\n",
       "  \"print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments['location'])\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)\",\n",
       "  'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# Create the connection to the OpenAI API using the API key\\nclient = OpenAI(api_key=os.environ.get(\\'OPENAI_API_KEY\\'))\\n\\n# Create a prompt for the user to enter a message\\nprompt = input(\"Enter a message: \")\\n\\n# Create a list of messages to send to the API\\nmessage_list = [\\n    {\\n        \"role\": \"system\", \\n        \"content\": \"You are a helpful assistant.\"\\n    },\\n    {\\n        \"role\": \"user\", \\n        \"content\": prompt\\n    }\\n]',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options,\\n    temperature=0.0\\n)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options,\\n    # temperature=0.0\\n)',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# This is the whole response object from the API\\nprint(completion + \\'\\\\n\\')\\n# Then we look at the choices that the model made\\nprint(completion.choices + \\'\\\\n\\')\\n# We look at the message details that the model created\\nprint(completion.choices[0].message + \\'\\\\n\\')\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content + \\'\\\\n\\')\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       "  'globals()',\n",
       "  '# Make variables for the natural language response or the tool call object that the model will generate\\nanswer = completion.choices[0].message.content\\ntool_calls = completion.choices[0].message.tool_calls\\n\\n# This code works because OpenAI\\'s models respond with EITHER a natural language response or a function call. There are other models that can generate both in the same response object. \\n\\n# If tool_calls isn\\'t empty (for natural language responses, it will be \\'None\\')\\nif tool_calls:\\n    # For every tool call in the tool calls object (there usually is only one, but there could be more with parallel function calling)\\n    for tool_call in tool_calls:\\n        # Finds the function called and the arguments passed to the function from the API response\\n        function_called = tool_call.function.name\\n        function_args = tool_call.function.arguments\\n        # Converts the arguments to a JSON object instead of a JSON string\\n        function_args_dict = json.loads(function_args)\\n        # Calls the function with the arguments\\n        # globals() is a Python native function that returns a dictionary with the global variables (including functions)\\n        if function_called in globals():\\n            result = eval(f\"{function_called}(**function_args_dict)\")\\n        else:\\n            result = \"Function not found. Please try again.\"\\n        print(result)\\nelse:\\n    print(answer)',\n",
       "  'globals()\\nf\"{function_called}(**function_args_json)\"',\n",
       "  'globals()'],\n",
       " 'Out': {37: {...}, 39: 'get_current_weather(**function_args_json)'},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x0000015B0DE55EE0>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x15b0cb48980>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x15b0cb48980>,\n",
       " 'open': <function _io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       " '_': 'get_current_weather(**function_args_json)',\n",
       " '__': {...},\n",
       " '___': '',\n",
       " '__vsc_ipynb_file__': 'c:\\\\Users\\\\Patrick\\\\Desktop\\\\PROJECTS\\\\Python Lesson Plan\\\\AI_CLASS_7\\\\function_calling_api.ipynb',\n",
       " '_i': 'globals()\\nf\"{function_called}(**function_args_json)\"',\n",
       " '_ii': '# Make variables for the natural language response or the tool call object that the model will generate\\nanswer = completion.choices[0].message.content\\ntool_calls = completion.choices[0].message.tool_calls\\n\\n# This code works because OpenAI\\'s models respond with EITHER a natural language response or a function call. There are other models that can generate both in the same response object. \\n\\n# If tool_calls isn\\'t empty (for natural language responses, it will be \\'None\\')\\nif tool_calls:\\n    # For every tool call in the tool calls object (there usually is only one, but there could be more with parallel function calling)\\n    for tool_call in tool_calls:\\n        # Finds the function called and the arguments passed to the function from the API response\\n        function_called = tool_call.function.name\\n        function_args = tool_call.function.arguments\\n        # Converts the arguments to a JSON object instead of a JSON string\\n        function_args_dict = json.loads(function_args)\\n        # Calls the function with the arguments\\n        # globals() is a Python native function that returns a dictionary with the global variables (including functions)\\n        if function_called in globals():\\n            result = eval(f\"{function_called}(**function_args_dict)\")\\n        else:\\n            result = \"Function not found. Please try again.\"\\n        print(result)\\nelse:\\n    print(answer)',\n",
       " '_iii': 'globals()',\n",
       " '_i1': '%pip install python-dotenv\\n%pip install openai',\n",
       " '_exit_code': 0,\n",
       " '_i2': 'from dotenv import load_dotenv\\nfrom openai import OpenAI\\nimport os, json\\n\\nload_dotenv(\"api.env\")  # could pass in the path of the .env file in the arguments\\nprint(os.getenv(\"OPENAI_API_KEY\"))',\n",
       " 'load_dotenv': <function dotenv.main.load_dotenv(dotenv_path: Union[str, ForwardRef('os.PathLike[str]'), NoneType] = None, stream: Optional[IO[str]] = None, verbose: bool = False, override: bool = False, interpolate: bool = True, encoding: Optional[str] = 'utf-8') -> bool>,\n",
       " 'OpenAI': openai.OpenAI,\n",
       " 'os': <module 'os' (frozen)>,\n",
       " 'json': <module 'json' from 'c:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\json\\\\__init__.py'>,\n",
       " '_i3': 'def get_current_weather(location, unit=\"fahrenheit\"):\\n    return f\"The current weather in {location} is 80 degrees {unit}.\"\\n\\ntool_options = [\\n    {\\n        \"type\": \"function\",\\n        \"function\": {\\n            \"name\": \"get_current_weather\",\\n            \"description\": \"Get the current weather in a given location\",\\n            \"parameters\": {\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"location\": {\\n                        \"type\": \"int\",\\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\\n                    },\\n                    \"unit\": {\\n                        \"type\": \"string\",\\n                        \"enum\": [\"celsius\", \"fahrenheit\"]\\n                    },\\n                },\\n                \"required\": [\"location\"],\\n            },\\n        }\\n    }\\n]',\n",
       " 'get_current_weather': <function __main__.get_current_weather(location, unit='fahrenheit')>,\n",
       " 'tool_options': [{'type': 'function',\n",
       "   'function': {'name': 'get_current_weather',\n",
       "    'description': 'Get the current weather in a given location',\n",
       "    'parameters': {'type': 'object',\n",
       "     'properties': {'location': {'type': 'string',\n",
       "       'description': 'The city and state, e.g. San Francisco, CA'},\n",
       "      'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}},\n",
       "     'required': ['location']}}}],\n",
       " '_i4': '# Create the connection to the OpenAI API using the API key\\nclient = OpenAI(api_key=os.environ.get(\\'OPENAI_API_KEY\\'))\\n\\n# Create a prompt for the user to enter a message\\nprompt = input(\"Enter a message: \")\\n\\n# Create a list of messages to send to the API\\nmessage_list = [\\n    {\\n        \"role\": \"system\", \\n        \"content\": \"You are a helpful assistant.\"\\n    },\\n    {\\n        \"role\": \"user\", \\n        \"content\": prompt\\n    }\\n]',\n",
       " 'client': <openai.OpenAI at 0x15b0ea429c0>,\n",
       " 'prompt': 'What is the weather in Miami?',\n",
       " 'message_list': [{'role': 'system',\n",
       "   'content': 'You are a helpful assistant.'},\n",
       "  {'role': 'user', 'content': 'What is the weather in Miami?'}],\n",
       " '_i5': 'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tools=tool_options,\\n    tool_choice=\"auto\"\\n)',\n",
       " '_i6': 'print(completion)',\n",
       " '_i7': 'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       " '_i8': '# This is the actual function that the model\\'s response object will call\\n# Notice how unit is an optional parameter (already initialized to \"fahrenheit\") \\ndef get_current_weather(location, unit=\"fahrenheit\"):\\n    # Usually there would be more code here to get the weather data\\n    # But for now, we\\'ll just return a set string saying that it is 80 degrees (possibly) fahrenheit at every location you give it\\n    return f\"The current weather in {location} is 80 degrees {unit}.\"\\n\\n# This is the list of dictionaries holding the natural language description of the function and the function\\'s arguments\\n# This object has a specific schema (look in the notes above this codeblock) that needs to be followed \\ntool_options = [\\n    {\\n        \"type\": \"function\",\\n        \"function\": {\\n            \"name\": \"get_current_weather\",\\n            \"description\": \"Get the current weather in a given location\",\\n            \"parameters\": {\\n                \"type\": \"object\",\\n                \"properties\": {\\n                    \"location\": {\\n                        \"type\": \"string\",\\n                        \"description\": \"The city and state, e.g. San Francisco, CA\", # Notice how we give the model an example of the string that it should generate for this argument\\n                    },\\n                    \"unit\": {\\n                        \"type\": \"string\",\\n                        \"enum\": [\"celsius\", \"fahrenheit\"] # Enum allows us to give a list of options the model can choose from instead of generating it\\'s own value\\n                    },\\n                },\\n                \"required\": [\"location\"], # We set \\'location\\' to be required and not \\'unit\\' because unit is an optional argument that is initialized to \\'fahrenheit\\' and the function will still properly run without it in the arguments.\\n            },\\n        }\\n    }\\n]',\n",
       " '_i9': 'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       " 'completion': ChatCompletion(id='chatcmpl-9eo03g2eAesC4fTWzbdCPjCUhxIvH', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_zlXBa0xIAhScH00ae9Hy0ypp', function=Function(arguments='{\"location\":\"Miami\",\"unit\":\"celsius\"}', name='get_current_weather'), type='function')]))], created=1719512347, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=20, prompt_tokens=87, total_tokens=107)),\n",
       " '_i10': 'print(completion)',\n",
       " '_i11': 'print(completion)\\n\\nwhat = json.load(completion)\\nwhat',\n",
       " '_i12': 'print(completion)\\n\\nwhat = json.dunp(completion)\\nwhat',\n",
       " '_i13': 'print(completion)\\n\\nwhat = json.dunps(completion)\\nwhat',\n",
       " '_i14': 'print(completion)\\n\\nwhat = json.dump(completion)\\nwhat',\n",
       " '_i15': 'print(completion)\\n\\nwhat = json.dumps(completion)\\nwhat',\n",
       " '_i16': 'print(completion)',\n",
       " '_i17': 'print(completion)\\nprint(completion.choices)',\n",
       " '_i18': 'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0])\\nprint(completion.choices[0].message)',\n",
       " '_i19': 'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0])\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)',\n",
       " '_i20': 'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)',\n",
       " '_i21': 'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\npritn(completion.choices[0].message.tool_calls[0].function.arguments)',\n",
       " '_i22': 'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)',\n",
       " '_i23': 'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)',\n",
       " 'arguments': {'location': 'Miami', 'unit': 'celsius'},\n",
       " '_i24': \"print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments['location'])\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)\",\n",
       " '_i25': 'print(completion)\\nprint(completion.choices)\\nprint(completion.choices[0].message)\\nprint(completion.choices[0].message.content)\\nprint(completion.choices[0].message.tool_calls)\\nprint(completion.choices[0].message.tool_calls[0].function)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments)',\n",
       " '_i26': '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       " '_i27': '# Create the connection to the OpenAI API using the API key\\nclient = OpenAI(api_key=os.environ.get(\\'OPENAI_API_KEY\\'))\\n\\n# Create a prompt for the user to enter a message\\nprompt = input(\"Enter a message: \")\\n\\n# Create a list of messages to send to the API\\nmessage_list = [\\n    {\\n        \"role\": \"system\", \\n        \"content\": \"You are a helpful assistant.\"\\n    },\\n    {\\n        \"role\": \"user\", \\n        \"content\": prompt\\n    }\\n]',\n",
       " '_i28': 'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options\\n)',\n",
       " '_i29': '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       " '_i30': 'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options,\\n    temperature=0.0\\n)',\n",
       " '_i31': '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       " '_i32': 'completion = client.chat.completions.create(\\n    model=\"gpt-3.5-turbo\",\\n    messages=message_list,\\n    tool_choice=\"auto\",\\n    tools=tool_options,\\n    # temperature=0.0\\n)',\n",
       " '_i33': '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       " '_i34': '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       " '_i35': '# This is the whole response object from the API\\nprint(completion + \\'\\\\n\\')\\n# Then we look at the choices that the model made\\nprint(completion.choices + \\'\\\\n\\')\\n# We look at the message details that the model created\\nprint(completion.choices[0].message + \\'\\\\n\\')\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content + \\'\\\\n\\')\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       " '_i36': '# This is the whole response object from the API\\nprint(completion)\\n# Then we look at the choices that the model made\\nprint(completion.choices)\\n# We look at the message details that the model created\\nprint(completion.choices[0].message)\\n# We look at the natural language response from the model (this will be \\'None\\' if it makes a function call)\\nprint(completion.choices[0].message.content)\\n# We look at the tool calls that the model decided to make\\nprint(completion.choices[0].message.tool_calls)\\n# We look at the first tool call and the function details \\nprint(completion.choices[0].message.tool_calls[0].function)\\n# We look at the function name that the model picked out of the list of function descriptions it was provided (tool_options)\\nprint(completion.choices[0].message.tool_calls[0].function.name)\\n# We look at the function\\'s arguments that the model chose. This is formatted as a JSON but is currently a string.\\nprint(completion.choices[0].message.tool_calls[0].function.arguments)\\n\\n# We use the \\'json\\' library we imported at the start to conver the string into an actual JSON object which we can then navigate like a dictionary.\\narguments = json.loads(completion.choices[0].message.tool_calls[0].function.arguments)\\nprint(arguments[\"location\"])',\n",
       " '_i37': 'globals()',\n",
       " '_37': {...},\n",
       " '_i38': '# Make variables for the natural language response or the tool call object that the model will generate\\nanswer = completion.choices[0].message.content\\ntool_calls = completion.choices[0].message.tool_calls\\n\\n# This code works because OpenAI\\'s models respond with EITHER a natural language response or a function call. There are other models that can generate both in the same response object. \\n\\n# If tool_calls isn\\'t empty (for natural language responses, it will be \\'None\\')\\nif tool_calls:\\n    # For every tool call in the tool calls object (there usually is only one, but there could be more with parallel function calling)\\n    for tool_call in tool_calls:\\n        # Finds the function called and the arguments passed to the function from the API response\\n        function_called = tool_call.function.name\\n        function_args = tool_call.function.arguments\\n        # Converts the arguments to a JSON object instead of a JSON string\\n        function_args_dict = json.loads(function_args)\\n        # Calls the function with the arguments\\n        # globals() is a Python native function that returns a dictionary with the global variables (including functions)\\n        if function_called in globals():\\n            result = eval(f\"{function_called}(**function_args_dict)\")\\n        else:\\n            result = \"Function not found. Please try again.\"\\n        print(result)\\nelse:\\n    print(answer)',\n",
       " 'answer': None,\n",
       " 'tool_calls': [ChatCompletionMessageToolCall(id='call_zlXBa0xIAhScH00ae9Hy0ypp', function=Function(arguments='{\"location\":\"Miami\",\"unit\":\"celsius\"}', name='get_current_weather'), type='function')],\n",
       " 'tool_call': ChatCompletionMessageToolCall(id='call_zlXBa0xIAhScH00ae9Hy0ypp', function=Function(arguments='{\"location\":\"Miami\",\"unit\":\"celsius\"}', name='get_current_weather'), type='function'),\n",
       " 'function_called': 'get_current_weather',\n",
       " 'function_args': '{\"location\":\"Miami\",\"unit\":\"celsius\"}',\n",
       " 'function_args_dict': {'location': 'Miami', 'unit': 'celsius'},\n",
       " 'result': 'The current weather in Miami is 80 degrees celsius.',\n",
       " '_i39': 'globals()\\nf\"{function_called}(**function_args_json)\"',\n",
       " '_39': 'get_current_weather(**function_args_json)',\n",
       " '_i40': 'globals()'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is an OpenAI example of getting a natural language response with the data returned from the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Example dummy function hard coded to return the same weather\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# In production, this could be your backend API or an external API\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_weather\u001b[39m(location, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfahrenheit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        print(messages)\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response.choices[0].message.content\n",
    "print(run_conversation())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
