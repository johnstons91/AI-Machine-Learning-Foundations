{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "* a framework that is focused on building applications that implement LLMs\n",
    "* we are going to use OpenAI models within LangChain and utilize some helpful tools that are a bit more tailored for the user experience\n",
    "\n",
    "### First lets setup our API connection with OpenAI\n",
    "* The only difference here, is that we are giving our API key to LangChain's OpenAI object (langchain is our middle man)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"C:\\\\Users\\\\Patrick\\\\Desktop\\\\PROJECTS\\\\Python Lesson Plan\\\\AI_CLASS_7\\\\api.env\")  # could pass in the path of the .env file in the arguments\n",
    "openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
    "# print(openai_api_key)\n",
    "# We also call the model here\n",
    "llm = ChatOpenAI(api_key=openai_api_key, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the .invoke() function to make an API call\n",
    "* So much easier\n",
    "* All that's needed is the prompt\n",
    "* The response object looks very similar to OpenAI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Tell me a joke\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "* We can build out more detailed prompts by using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Why did the AI break up with his internet girlfriend?\\n\\nBecause he couldn't handle her bandwidth!\", response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 25, 'total_tokens': 44}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0a4c3257-54c0-49e2-9165-ed0cf38fc77a-0', usage_metadata={'input_tokens': 25, 'output_tokens': 19, 'total_tokens': 44})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using ChatPromptTemplate we can create a chat history that is simpler than OpenAI's format\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class comedian who focuses on AI.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt | llm \n",
    "chain.invoke({\"input\": \"Tell me a joke\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the robot go on a diet?\\n\\nBecause it had too many bytes!', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 23, 'total_tokens': 39}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-8bdf70de-4be0-4692-bc3c-42ee6c196e5d-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can make it more task specific and user friendly\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class comedian.\"),\n",
    "    # We create the variable 'topic' here that the user can fill in\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "chain = prompt | llm \n",
    "# This is where the user will fill in the variable 'topic'\n",
    "chain.invoke({\"topic\": \"AI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Prompt Templates\n",
    "* Separates the core prompt from specific details \n",
    "    * keeps your code cleaner and more modular\n",
    "* the real power lies in calling functions to handle calculations as partial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'07/13/2024'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Using the 'datetime' library, we made a function that fetches the current date from the device\n",
    "def get_datetime():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "# lets run it and test it out\n",
    "get_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was 07/13/2024 such a great day? Because it was the only day when everyone agreed that Mondays aren't so bad after all!\n"
     ]
    }
   ],
   "source": [
    "# This looks like a normal prompt template but with the partial_variables parameter added and set date to the function to get the date\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\",\n",
    "    input_variables=[\"adjective\"],\n",
    "    # This makes the prompt template essentially update the date everytime the model gets prompted\n",
    "    partial_variables={\"date\": get_datetime},\n",
    ")\n",
    "# Create the chain with the prompt template and the model\n",
    "chain = prompt | llm\n",
    "# .invoke() to prompt the model and we add our adjective - this is what the user would put into this hypothetical AI application\n",
    "response = chain.invoke({\"adjective\": \"funny\"})\n",
    "# Display the output by just using .content\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot Prompt Template\n",
    "* using the FewShotPromptTemplate object to incorporate examples for few shot prompt engineering\n",
    "* examples and example_prompt parameters are required in the FewShotPromptTemplate\n",
    "    * The object will pass the examples through the example_prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of craigslist born?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate Answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate Answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate Answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate Answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "* Whenever we make an API call using the .invoke() function, we get an AIMessage response object returned back to us\n",
    "* When building applications, all we need to display is the generated content of the response object. \n",
    "    * The object does hold some helpful information that can be used as well, but that is a case by case basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why was the equal sign so humble?\\n\\nBecause he knew he wasn't less than or greater than anyone else!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we use the StrOutputParser to get the output as a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "# We can just add it in the chain to be incorporated in the .invoke() function\n",
    "# Uses the same prompt template as before (scroll up and find the prompt variable)\n",
    "chain = prompt | llm | output_parser\n",
    "# Notice how the output is now just the content as a string\n",
    "chain.invoke({\"topic\": \"math\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other Output Parsers available in LangChain.\n",
    "Accessible here: https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joyful', 'delighted', 'content', 'cheerful', 'elated']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use the CommaSeparatedListOutputParser to get the output as a list\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "# We take the format instructions from the output_parser to feed into the prompt template. This allows the model to understand the format of the input when generating it\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# We create a new prompt template that takes in the format instructions\n",
    "# We use the partial variables to pass in the format instructions because the format instructions are not known until the output_parser is created\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five synonyms for {word}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# We create the chain to incorporate the prompt, the model, and the output parser\n",
    "chain = prompt | llm | output_parser\n",
    "# We invoke the chain with the variable 'word' filled in\n",
    "chain.invoke({\"word\": \"happy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Chain together a prompt template, an OpenAI model, and an output parser together to make a recipe generator for any meal the user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Chaining\n",
    "* a technique that breaks down a prompt into multiple subtasks\n",
    "* typically used for longer generations or complex tasks that need more direction in its execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Tragedy at Sunset on the Beach\" follows the story of a group of friends who gather for a relaxing evening by the ocean. As the sun sets and tensions rise, long-buried secrets and conflicts come to light, leading to a tragic event that changes their lives forever. Set against the backdrop of a picturesque beach, this play explores themes of friendship, betrayal, and the consequences of our actions as the characters struggle to come to terms with the heartbreaking events that unfold before them.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\"Tragedy at Sunset on the Beach\" is a gripping and emotionally charged play that delves into the complexities of human relationships and the devastating consequences of unresolved conflicts. Set against the stunning backdrop of a beach at sunset, the play unfolds with a sense of impending doom as long-buried secrets come to light and tensions reach a boiling point.\n",
      "\n",
      "The talented ensemble cast delivers powerful performances, capturing the raw emotions and turmoil of their characters as they grapple with betrayal, loss, and the weight of their past actions. The playwright skillfully weaves together moments of tension, humor, and heartbreak, keeping the audience on the edge of their seats until the heartbreaking climax.\n",
      "\n",
      "As the characters struggle to come to terms with the tragic events that unfold, the audience is left reflecting on the fragility of relationships and the importance of confronting our demons before it's too late. \"Tragedy at Sunset on the Beach\" is a thought-provoking and poignant exploration of the human experience, leaving a lasting impact on all who witness its powerful storytelling.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# This is an LLMChain to write a synopsis given a title of a play.\n",
    "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
    "\n",
    "Title: {title}\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# This is an LLMChain to write a review of a play given a synopsis.\n",
    "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
    "\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
    "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# This is the overall chain where we run these two chains in sequence.\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[synopsis_chain, review_chain], verbose=True\n",
    ")\n",
    "\n",
    "review = overall_chain.run(\"Tragedy at sunset on the beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "* synonymous with the term function calling\n",
    "* allows the model being used in the application to choose a function from the list provided instead of generating a natural language response\n",
    "* define our own tools using the ‘@tool’ decorator\n",
    "    * there has to be a docstring that describes the function in natural language for the model to understand the proper use of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_M25FBFSr4vtmlORIFU4XmR0h', 'function': {'arguments': '{\"x\": 257, \"y\": 243}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_5GjVtI1V5SWRZ23aJHNjtcAE', 'function': {'arguments': '{\"x\": 223, \"y\": 3}', 'name': 'multiply'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 112, 'total_tokens': 161}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-2fa80aa9-ff08-4975-b58f-3bf6d75a7159-0' tool_calls=[{'name': 'add', 'args': {'x': 257, 'y': 243}, 'id': 'call_M25FBFSr4vtmlORIFU4XmR0h'}, {'name': 'multiply', 'args': {'x': 223, 'y': 3}, 'id': 'call_5GjVtI1V5SWRZ23aJHNjtcAE'}]\n",
      "[{'name': 'add', 'args': {'x': 257, 'y': 243}, 'id': 'call_M25FBFSr4vtmlORIFU4XmR0h'}, {'name': 'multiply', 'args': {'x': 223, 'y': 3}, 'id': 'call_5GjVtI1V5SWRZ23aJHNjtcAE'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool # tool decorator\n",
    "def add(x: int, y: int) -> int:\n",
    "    '''Adds two numbers together.''' # Docstring \n",
    "    return x + y\n",
    "\n",
    "@tool\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    '''Multiplies two numbers together.'''\n",
    "    return x * y\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "prompt = \"What is 257 + 243? Also, what is 223 * 3?\"\n",
    "result = llm_with_tools.invoke(prompt)\n",
    "print(result)\n",
    "print(result.tool_calls)\n",
    "# some models may also add content to explain the reasoning behind the function calls\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a list of available tools: https://python.langchain.com/v0.2/docs/integrations/tools/\n",
    "\n",
    "Here are a list of toolkits that hold alike tools: https://python.langchain.com/v0.2/docs/integrations/toolkits/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Write a tool calling chain that converts USD to two different currencies (ex: the euro and Japanese yen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "* An agent chooses the sequence of actions to take using a reasoning engine\n",
    "* This allows the model to take control and complete more versatile tasks instead of being hard coded down a path in a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'x': 257, 'y': 243}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m500\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'x': 500, 'y': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m1500\u001b[0m\u001b[32;1m\u001b[1;3mThe sum of 257 and 243 is 500. When this result is multiplied by 3, the answer is 1500.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is 257 + 243? Also, what is that result mutliplied by 3?',\n",
       " 'output': 'The sum of 257 and 243 is 500. When this result is multiplied by 3, the answer is 1500.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are mathematical genius that can answer any math question.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What is 257 + 243? Also, what is that result mutliplied by 3?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is more information on Agents: https://python.langchain.com/v0.1/docs/modules/agents/concepts/ \n",
    "\n",
    "Here is a list of Agent types available in LangChain: https://python.langchain.com/v0.1/docs/modules/agents/agent_types/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Use one (or more) of the simple to use tools that LangChain provides to create an agent. (AlphaVantage, Brave Search, Dall-E Image Generator, OpenWeatherMap, SerpAPI, WolframAlpha, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
