{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbh2lKHPDY9b",
        "outputId": "6fd613bf-3230-4918-8cd1-11c4e2d98f67"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "eROYhHF9DhCI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XMWwuHs0DP4W"
      },
      "outputs": [],
      "source": [
        "# List of pretrained tokenizer names\n",
        "tokenizer_names = [\n",
        "    \"bert-base-uncased\", # BERT base model with uncased vocab\n",
        "    \"gpt2\", # GPT-2 model\n",
        "    \"openai-gpt\" # OpenAI GPT model\n",
        "]\n",
        "\n",
        "# Sample paragraph of text\n",
        "text = \"This is a quick test of the tokenizers. We're testing the tokenization of this text! This is to see if there are differences between the tokenizers. Do you think big words like supercalifragilisticexpialidocious will be interesting to see? Let's find out!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = tokenizer_names[0]\n",
        "name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3_9Nk3PpD6Dy",
        "outputId": "fbeee4fa-2396-4e6b-ed38-60ed85003d13"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bert-base-uncased'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nTokenizing with {name} tokenizer:\\n\")\n",
        "\n",
        "# Initiaizing the tokenizer for the specific model\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "# Split the text into tokens\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Convert the tokens to ids - commonly referenced as tokens as well\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# From those ids we can reconstruct the text\n",
        "# This is part of processing the output from the model to be interpretable by the user\n",
        "text_reconstructed = tokenizer.decode(ids)\n",
        "\n",
        "print(\"Tokens:\", tokens, '\\n')\n",
        "print(\"IDs:\", ids, '\\n')\n",
        "print(\"Reconstructed Text:\", text_reconstructed, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6IZrgtrDxIy",
        "outputId": "9bcb1391-24a2-411b-f7c6-739c6cb06924"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenizing with bert-base-uncased tokenizer:\n",
            "\n",
            "Tokens: ['this', 'is', 'a', 'quick', 'test', 'of', 'the', 'token', '##izer', '##s', '.', 'we', \"'\", 're', 'testing', 'the', 'token', '##ization', 'of', 'this', 'text', '!', 'this', 'is', 'to', 'see', 'if', 'there', 'are', 'differences', 'between', 'the', 'token', '##izer', '##s', '.', 'do', 'you', 'think', 'big', 'words', 'like', 'super', '##cal', '##if', '##rag', '##ilis', '##tic', '##ex', '##pia', '##lid', '##oc', '##ious', 'will', 'be', 'interesting', 'to', 'see', '?', 'let', \"'\", 's', 'find', 'out', '!'] \n",
            "\n",
            "IDs: [2023, 2003, 1037, 4248, 3231, 1997, 1996, 19204, 17629, 2015, 1012, 2057, 1005, 2128, 5604, 1996, 19204, 3989, 1997, 2023, 3793, 999, 2023, 2003, 2000, 2156, 2065, 2045, 2024, 5966, 2090, 1996, 19204, 17629, 2015, 1012, 2079, 2017, 2228, 2502, 2616, 2066, 3565, 9289, 10128, 29181, 24411, 4588, 10288, 19312, 21273, 10085, 6313, 2097, 2022, 5875, 2000, 2156, 1029, 2292, 1005, 1055, 2424, 2041, 999] \n",
            "\n",
            "Reconstructed Text: this is a quick test of the tokenizers. we're testing the tokenization of this text! this is to see if there are differences between the tokenizers. do you think big words like supercalifragilisticexpialidocious will be interesting to see? let's find out! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\""
      ],
      "metadata": {
        "id": "yyX3bBt4FI-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}